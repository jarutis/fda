{"metadata":{"name":"Sentimentas","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/home/jjarutis/.m2/repository","customRepos":null,"customDeps":["org.apache.avro % avro-mapred % 1.7.7","com.twitter % parquet-hive-bundle % 1.6.0","com.databricks:spark-avro_2.10:1.0.0","com.databricks:spark-csv_2.11:1.0.3","org.apache.spark % spark-mllib_2.10 % 1.4.0","org.deeplearning4j % dl4j-spark-nlp % 0.0.3.3.5.alpha2-SNAPSHOT","org.deeplearning4j % dl4j-spark % 0.0.3.3.5.alpha2-SNAPSHOT","org.nd4j % nd4j-java % 0.0.3.5.5.6-SNAPSHOT"],"customImports":null,"customSparkConf":{"SparkDl4jMultiLayer.AVERAGE_EACH_ITERATION":"false","Word2VecPerformer.NEGATIVE":"0","spark.akka.frameSize":"100","Word2VecPerformer.VECTOR_LENGTH":"300","spark.app.name":"Sentimentai","spark.master":"yarn-client","spark.executor.memory":"3G","spark.executor.instances":"10","spark.sql.shuffle.partitions":"400","spark.yarn.jar":"hdfs:///user/jjarutis/spark-assembly-1.4.0-hadoop2.5.0-cdh5.3.2.jar"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Deeplearning4j"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.sql.hive.HiveContext\nval hiveCtx = new HiveContext(sc)","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.sql.hive.HiveContext\nhiveCtx: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@4d0fde48\n"},{"metadata":{},"data":{"text/html":"org.apache.spark.sql.hive.HiveContext@4d0fde48"},"output_type":"execute_result","execution_count":1}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"hiveCtx.sql(\"use jarucio\")\nval feedback = hiveCtx.table(\"us__user_feedback\")\n  .select(\"feedback\")\n  .filter(\"length(feedback) > 40 AND feedback IS NOT NULL\")\n  .rdd.map(_.getString(0).toLowerCase.replaceAll(\"[^a-z ]\", \"\"))","outputs":[{"name":"stdout","output_type":"stream","text":"feedback: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at map at <console>:49\n"},{"metadata":{},"data":{"text/html":"MapPartitionsRDD[17] at map at &lt;console&gt;:49"},"output_type":"execute_result","execution_count":3}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"feedback.first","outputs":[{"name":"stdout","output_type":"stream","text":"res2: String = can you please message me once you make the payment to my paypal once i recieve the payment i shall ship you the dresses where do you live\n"},{"metadata":{},"data":{"text/html":"can you please message me once you make the payment to my paypal once i recieve the payment i shall ship you the dresses where do you live"},"output_type":"execute_result","execution_count":4}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.deeplearning4j.spark.models.embeddings.word2vec.Word2Vec\nimport org.deeplearning4j.spark.models.embeddings.word2vec.Word2VecPerformer","outputs":[{"name":"stdout","output_type":"stream","text":"import org.deeplearning4j.spark.models.embeddings.word2vec.Word2Vec\nimport org.deeplearning4j.spark.models.embeddings.word2vec.Word2VecPerformer\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val vec = new Word2Vec()\nval table = vec.train(feedback)","outputs":[{"name":"stdout","output_type":"stream","text":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 4 times, most recent failure: Lost task 2.3 in stage 4.0 (TID 22, hz-bd-hdp16.vinted.net): java.lang.NullPointerException\n\tat org.deeplearning4j.spark.models.embeddings.word2vec.SentenceBatch.iterateSample(SentenceBatch.java:171)\n\tat org.deeplearning4j.spark.models.embeddings.word2vec.SentenceBatch.skipGram(SentenceBatch.java:97)\n\tat org.deeplearning4j.spark.models.embeddings.word2vec.SentenceBatch.trainSentence(SentenceBatch.java:71)\n\tat org.deeplearning4j.spark.models.embeddings.word2vec.SentenceBatch.call(SentenceBatch.java:55)\n\tat org.deeplearning4j.spark.models.embeddings.word2vec.SentenceBatch.call(SentenceBatch.java:42)\n\tat org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1027)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$28.apply(RDD.scala:870)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$28.apply(RDD.scala:870)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"# Spark mllib"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":7}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val word2vec = new Word2Vec()\nval model = word2vec.fit(feedback.map(line => line.split(\" +\").toSeq))","outputs":[{"name":"stdout","output_type":"stream","text":"word2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@6aeab651\nmodel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@6b5fe094\n"},{"metadata":{},"data":{"text/html":"org.apache.spark.mllib.feature.Word2VecModel@6b5fe094"},"output_type":"execute_result","execution_count":8}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"model.findSynonyms(\"feedback\", 5).map(println(_))","outputs":[{"name":"stdout","output_type":"stream","text":"(review,2.5387413046046556)\n(negative,2.3521037985042934)\n(neutral,2.1850603806623643)\n(leaving,2.1666648658048175)\n(hate,2.083232351849998)\nres3: Array[Unit] = Array((), (), (), (), ())\n"},{"metadata":{},"data":{"text/html":"<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon7fe3406d01e32aef53486467ec7ff934&quot;,&quot;dataInit&quot;:[{},{},{},{},{}],&quot;genId&quot;:&quot;642277326&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[],\"nrow\":5,\"shown\":5,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div></div></div></div>"},"output_type":"execute_result","execution_count":9}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}